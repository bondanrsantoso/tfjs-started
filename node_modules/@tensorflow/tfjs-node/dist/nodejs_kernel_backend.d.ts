import { BackendTimingInfo, DataType, KernelBackend, Rank, ShapeMap, Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D } from '@tensorflow/tfjs-core';
import { Conv2DInfo } from '@tensorflow/tfjs-core/dist/ops/conv_util';
import { TFJSBinding } from './tfjs_binding';
export declare class NodeJSKernelBackend implements KernelBackend {
    private binding;
    private tensorMap;
    constructor(binding: TFJSBinding);
    private getTFDType(dataType);
    private createOutputTensor(metadata);
    private getInputTensorIds(tensors);
    private createReductionOpAttrs(tensor);
    private createTypeOpAttr(attrName, dtype);
    private executeSingleInput(name, input);
    private executeSingleOutput(name, opAttrs, inputs);
    private executeMultipleOutputs(name, opAttrs, inputs, numOutputs);
    dispose(): void;
    read(dataId: object): Promise<Float32Array | Int32Array | Uint8Array>;
    readSync(dataId: object): Float32Array | Int32Array | Uint8Array;
    disposeData(dataId: object): void;
    write(dataId: object, values: Float32Array | Int32Array | Uint8Array): void;
    register(dataId: object, shape: number[], dtype: DataType): void;
    matMul(a: Tensor2D, b: Tensor2D, transposeA: boolean, transposeB: boolean): Tensor2D;
    stridedSlice<T extends Tensor<Rank>>(x: T, begin: number[], end: number[], strides: number[], beginMask: number, endMask: number): T;
    slice<T extends Tensor>(x: T, begin: number[], size: number[]): T;
    reverse<T extends Tensor>(a: T, axis: number[]): T;
    concat(a: Tensor2D, b: Tensor2D): Tensor2D;
    neg<T extends Tensor>(a: T): T;
    add(a: Tensor, b: Tensor): Tensor;
    subtract(a: Tensor, b: Tensor): Tensor;
    multiply(a: Tensor, b: Tensor): Tensor;
    realDivide(a: Tensor<Rank>, b: Tensor<Rank>): Tensor<Rank>;
    floorDiv(a: Tensor<Rank>, b: Tensor<Rank>): Tensor<Rank>;
    divide(a: Tensor, b: Tensor): Tensor;
    sum(x: Tensor, axes: number[]): Tensor;
    argMin(x: Tensor, axis: number): Tensor;
    argMax(x: Tensor, axis: number): Tensor;
    equal(a: Tensor, b: Tensor): Tensor;
    notEqual(a: Tensor, b: Tensor): Tensor;
    less(a: Tensor, b: Tensor): Tensor;
    lessEqual(a: Tensor, b: Tensor): Tensor;
    greater(a: Tensor, b: Tensor): Tensor;
    greaterEqual(a: Tensor, b: Tensor): Tensor;
    logicalNot<T extends Tensor>(a: T): T;
    logicalAnd(a: Tensor, b: Tensor): Tensor;
    logicalOr(a: Tensor, b: Tensor): Tensor;
    where(condition: Tensor, a: Tensor, b: Tensor, dtype: DataType): Tensor;
    topKValues<T extends Tensor>(x: T, k: number): Tensor1D;
    topKIndices(x: Tensor, k: number): Tensor1D;
    min(x: Tensor, axes: number[]): Tensor;
    minimum(a: Tensor, b: Tensor): Tensor;
    max(x: Tensor, axes: number[]): Tensor;
    maximum(a: Tensor, b: Tensor): Tensor;
    ceil<T extends Tensor>(x: T): T;
    floor<T extends Tensor>(x: T): T;
    pow<T extends Tensor>(a: T, b: Tensor): T;
    exp<T extends Tensor>(x: T): T;
    log<T extends Tensor>(x: T): T;
    log1p<T extends Tensor>(x: T): T;
    sqrt<T extends Tensor>(x: T): T;
    square<T extends Tensor>(x: T): T;
    relu<T extends Tensor>(x: T): T;
    elu<T extends Tensor>(x: T): T;
    eluDer<T extends Tensor>(dy: T, y: T): T;
    selu<T extends Tensor>(x: T): T;
    int<T extends Tensor>(x: T): T;
    clip<T extends Tensor>(x: T, min: number, max: number): T;
    abs<T extends Tensor>(x: T): T;
    sigmoid<T extends Tensor>(x: T): T;
    sin<T extends Tensor>(x: T): T;
    cos<T extends Tensor>(x: T): T;
    tan<T extends Tensor>(x: T): T;
    asin<T extends Tensor>(x: T): T;
    acos<T extends Tensor>(x: T): T;
    atan<T extends Tensor>(x: T): T;
    sinh<T extends Tensor>(x: T): T;
    cosh<T extends Tensor>(x: T): T;
    tanh<T extends Tensor>(x: T): T;
    mod(a: Tensor, b: Tensor): Tensor;
    round<T extends Tensor>(x: T): T;
    sign<T extends Tensor>(x: T): T;
    rsqrt<T extends Tensor>(x: T): T;
    reciprocal<T extends Tensor>(x: T): T;
    asinh<T extends Tensor>(x: T): T;
    acosh<T extends Tensor>(x: T): T;
    atanh<T extends Tensor>(x: T): T;
    erf<T extends Tensor>(x: T): T;
    squaredDifference(a: Tensor, b: Tensor): Tensor;
    expm1<T extends Tensor>(x: T): T;
    softplus<T extends Tensor>(x: T): T;
    atan2<T extends Tensor>(a: T, b: T): T;
    step<T extends Tensor>(x: T, alpha: number): T;
    conv2d(x: Tensor4D, filter: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    conv2dDerInput(dy: Tensor4D, filter: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    conv2dDerFilter(x: Tensor4D, dy: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    depthwiseConv2DDerInput(dy: Tensor<Rank.R4>, filter: Tensor<Rank.R4>, convInfo: Conv2DInfo): Tensor4D;
    depthwiseConv2DDerFilter(x: Tensor<Rank.R4>, dY: Tensor<Rank.R4>, convInfo: Conv2DInfo): Tensor<Rank.R4>;
    depthwiseConv2D(input: Tensor4D, filter: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    maxPool(x: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    maxPoolBackprop(dy: Tensor4D, x: Tensor4D, y: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    avgPool(x: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    avgPoolBackprop(dy: Tensor4D, x: Tensor4D, convInfo: Conv2DInfo): Tensor4D;
    reshape<T extends Tensor, R extends Rank>(x: T, shape: ShapeMap[R]): Tensor<R>;
    cast<T extends Tensor>(x: T, dtype: DataType): T;
    tile<T extends Tensor>(x: T, reps: number[]): T;
    pad<T extends Tensor>(x: T, paddings: Array<[number, number]>, constantValue: number): T;
    transpose<T extends Tensor>(x: T, perm: number[]): T;
    gather<T extends Tensor>(x: T, indices: Tensor1D, axis: number): T;
    resizeBilinear(x: Tensor4D, newHeight: number, newWidth: number, alignCorners: boolean): Tensor4D;
    resizeBilinearBackprop(dy: Tensor<Rank.R4>, x: Tensor<Rank.R4>, alignCorners: boolean): Tensor<Rank.R4>;
    resizeNearestNeighbor(x: Tensor4D, newHeight: number, newWidth: number, alignCorners: boolean): Tensor4D;
    batchNormalization(x: Tensor4D, mean: Tensor1D | Tensor4D, variance: Tensor1D | Tensor4D, varianceEpsilon: number, scale?: Tensor1D | Tensor4D, offset?: Tensor1D | Tensor4D): Tensor4D;
    localResponseNormalization4D(x: Tensor4D, radius: number, bias: number, alpha: number, beta: number): Tensor4D;
    multinomial(logits: Tensor2D, normalized: boolean, numSamples: number, seed: number): Tensor2D;
    oneHot(indices: Tensor1D, depth: number, onValue: number, offValue: number): Tensor2D;
    cumsum(x: Tensor<Rank>, axis: number, exclusive: boolean, reverse: boolean): Tensor<Rank>;
    fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
    memory(): {
        unreliable: boolean;
    };
    time(f: () => void): Promise<BackendTimingInfo>;
    isNaN<T extends Tensor>(x: T): T;
}
